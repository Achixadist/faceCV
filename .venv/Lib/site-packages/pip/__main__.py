import cv2
import numpy as np
import time
import os


def initialize_camera():
    """Try to initialize camera with fallback options"""
    for camera_id in range(3):  # Try camera indices 0, 1, 2
        print(f"Attempting to open camera {camera_id}...")
        cap = cv2.VideoCapture(camera_id)
        if cap.isOpened():
            print(f"Successfully opened camera {camera_id}")
            return cap, camera_id

    # If we get here, no camera could be opened
    print("ERROR: Could not open any camera. Please check camera connections.")
    return None, -1


def load_face_detector():
    """Load appropriate face detector based on available models"""
    face_cascade = None
    net = None
    use_dnn_detector = False

    try:
        cascade_path = cv2.data.haarcascades + 'haarcascade_frontalface_default.xml'
        if os.path.exists(cascade_path):
            face_cascade = cv2.CascadeClassifier(cascade_path)
            if not face_cascade.empty():
                print("Haar cascade face detector loaded successfully")
            else:
                print("Error: Haar cascade file is invalid")
                face_cascade = None
        else:
            print(f"Error: Haar cascade file not found at {cascade_path}")
    except Exception as e:
        print(f"Error loading Haar cascade: {e}")

    try:
        # Check if the model files exist
        prototxt_path = "deploy.prototxt"
        model_path = "res10_300x300_ssd_iter_140000.caffemodel"

        if os.path.exists(prototxt_path) and os.path.exists(model_path):
            net = cv2.dnn.readNetFromCaffe(prototxt_path, model_path)
            use_dnn_detector = True
            print("DNN-based face detector loaded successfully")
        else:
            print(f"DNN model files not found. Looking for:")
            print(f"  - {os.path.abspath(prototxt_path)}")
            print(f"  - {os.path.abspath(model_path)}")
    except Exception as e:
        print(f"Error loading DNN detector: {e}")

    return face_cascade, net, use_dnn_detector


def apply_zoom(frame, zoom_factor, center_x=None, center_y=None):
    h, w = frame.shape[:2]

    # If no center specified, use the center of the image
    if center_x is None:
        center_x = w // 2
    if center_y is None:
        center_y = h // 2

    # Calculate dimensions for zoomed frame
    new_w = int(w / zoom_factor)
    new_h = int(h / zoom_factor)

    # Calculate top-left corner of crop area
    x1 = max(0, int(center_x - new_w / 2))
    y1 = max(0, int(center_y - new_h / 2))

    # Make sure we don't exceed the image dimensions
    x2 = min(w, x1 + new_w)
    y2 = min(h, y1 + new_h)

    # Recalculate x1, y1 if x2, y2 were clamped
    x1 = max(0, x2 - new_w)
    y1 = max(0, y2 - new_h)

    # Crop and resize
    cropped = frame[y1:y2, x1:x2]
    zoomed = cv2.resize(cropped, (w, h), interpolation=cv2.INTER_LINEAR)

    return zoomed


def main():
    print("Starting Face Tracking with Simulated Zoom...")

    # Load the pre-trained face detector
    face_cascade, net, use_dnn_detector = load_face_detector()

    if face_cascade is None and not use_dnn_detector:
        print("ERROR: No face detector could be loaded. Exiting.")
        return

    # Initialize the camera
    cap, camera_id = initialize_camera()
    if cap is None:
        return

    # Get the original frame dimensions
    ret, frame = cap.read()
    if not ret:
        print("Error: Could not read from camera.")
        cap.release()
        return

    original_height, original_width = frame.shape[:2]
    center_x, center_y = original_width // 2, original_height // 2
    print(f"Camera resolution: {original_width}x{original_height}")

    # Initialize tracking variables
    zoom_factor = 1.25  # Start with 1.25x zoom as requested
    target_zoom = zoom_factor

    # PID control variables for tracking
    prev_offset_x = 0
    prev_offset_y = 0
    integral_x = 0
    integral_y = 0

    # PID constants
    kp = 0.5
    ki = 0.05
    kd = 0.15

    # Parameters for zoom
    max_zoom = 3.0  # Max zoom level is 3x
    min_zoom = 1.0  # Min zoom level is 1x
    zoom_speed = 0.08  # Speed of zoom transitions

    # Movement threshold for face centering
    movement_threshold = 10

    # Variables for remembering initial face size
    initial_face_size = None
    initial_face_size_samples = []
    initial_samples_needed = 30  # Collect samples to get a stable initial size
    face_size_stabilization_counter = 0

    # Timing variables
    last_zoom_time = time.time() - 1.0
    zoom_interval = 0.15

    # Face tracking history for smoother movement
    face_positions = []
    max_history = 8

    # Current tracking point - where we're zooming to
    tracking_x = center_x
    tracking_y = center_y

    # Create a window
    cv2.namedWindow("Face Tracking with Zoom")

    consecutive_tracking_frames = 0
    frames_without_face = 0
    last_successful_tracking_time = time.time()

    print("Face tracking system initialized and ready")
    print("Press 'q' to exit, 'r' to reset, 'c' to calibrate")

    # Main processing loop
    while True:
        # Capture frame
        ret, original_frame = cap.read()
        if not ret:
            print("Error: Could not read frame. Camera may be disconnected.")
            break

        current_time = time.time()

        # Make a copy of the frame for display
        display_frame = original_frame.copy()

        # Initialize largest face variables
        largest_face_area = 0
        largest_face = None

        # Face detection using appropriate method
        if use_dnn_detector and net is not None:
            # DNN-based face detection (more robust)
            try:
                blob = cv2.dnn.blobFromImage(cv2.resize(original_frame, (300, 300)), 1.0, (300, 300),
                                             (104.0, 177.0, 123.0))
                net.setInput(blob)
                detections = net.forward()

                for i in range(detections.shape[2]):
                    confidence = detections[0, 0, i, 2]
                    if confidence > 0.5:  # Confidence threshold
                        box = detections[0, 0, i, 3:7] * np.array(
                            [original_width, original_height, original_width, original_height])
                        (x, y, x2, y2) = box.astype("int")
                        w = x2 - x
                        h = y2 - y

                        # Check for valid face dimensions
                        if w > 0 and h > 0:
                            face_area = w * h
                            if face_area > largest_face_area:
                                largest_face_area = face_area
                                largest_face = (x, y, w, h)
            except Exception as e:
                print(f"Error in DNN face detection: {e}")
                use_dnn_detector = False

        if not use_dnn_detector and face_cascade is not None:
            # Haar cascade detection
            try:
                gray = cv2.cvtColor(original_frame, cv2.COLOR_BGR2GRAY)
                gray = cv2.equalizeHist(gray)

                faces = face_cascade.detectMultiScale(
                    gray,
                    scaleFactor=1.1,
                    minNeighbors=5,
                    minSize=(30, 30),
                    flags=cv2.CASCADE_SCALE_IMAGE
                )

                for (x, y, w, h) in faces:
                    face_area = w * h
                    if face_area > largest_face_area:
                        largest_face_area = face_area
                        largest_face = (x, y, w, h)
            except Exception as e:
                print(f"Error in Haar cascade face detection: {e}")

        # Track and zoom based on face
        if largest_face is not None:
            frames_without_face = 0
            last_successful_tracking_time = current_time

            x, y, w, h = largest_face

            # Calculate face center
            face_center_x = x + w // 2
            face_center_y = y + h // 2

            # Add to position history for smoothing
            face_positions.append((face_center_x, face_center_y))
            if len(face_positions) > max_history:
                face_positions.pop(0)

            # Calculate smoothed face position
            total_weight = 0
            smoothed_x = 0
            smoothed_y = 0

            for i, pos in enumerate(face_positions):
                weight = i + 1
                smoothed_x += pos[0] * weight
                smoothed_y += pos[1] * weight
                total_weight += weight

            smoothed_x /= total_weight
            smoothed_y /= total_weight

            face_center_x = int(smoothed_x)
            face_center_y = int(smoothed_y)

            # Calculate face size (diagonal for better distance estimation)
            face_size = np.sqrt(w ** 2 + h ** 2)

            # Handle initial face size calibration
            if initial_face_size is None:
                # Collect samples for initial face size
                initial_face_size_samples.append(face_size)

                # Only use recent samples (last 30 frames)
                if len(initial_face_size_samples) > 30:
                    initial_face_size_samples.pop(0)

                # Wait until we have enough samples and face position is stable
                if len(initial_face_size_samples) >= initial_samples_needed:
                    face_size_stabilization_counter += 1

                    # Check if the face position is relatively stable
                    if face_size_stabilization_counter >= 15:  # 15 frames of stability
                        # Calculate average of recent samples as initial face size
                        initial_face_size = sum(initial_face_size_samples) / len(initial_face_size_samples)
                        print(f"Initial face size calibrated: {initial_face_size:.2f}")

                        # Set zoom to 1.25x for this initial face size
                        zoom_factor = 1.25
                        target_zoom = 1.25

            # Draw rectangle around the face
            cv2.rectangle(display_frame, (x, y), (x + w, y + h), (255, 0, 0), 2)
            cv2.circle(display_frame, (face_center_x, face_center_y), 5, (0, 255, 0), -1)
            cv2.circle(display_frame, (center_x, center_y), 5, (0, 0, 255), -1)
            cv2.rectangle(display_frame,
                          (center_x - movement_threshold, center_y - movement_threshold),
                          (center_x + movement_threshold, center_y + movement_threshold),
                          (255, 255, 0), 1)

            # Calculate offset from center
            offset_x = face_center_x - center_x
            offset_y = center_y - face_center_y  # Inverted for camera control

            # Display offset info
            cv2.putText(display_frame, f"Offset: ({offset_x:.1f}, {offset_y:.1f})", (10, 90),
                        cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)

            # Calculate PID control values
            p_x = kp * offset_x
            p_y = kp * offset_y
            integral_x = max(-50, min(50, integral_x + ki * offset_x))
            integral_y = max(-50, min(50, integral_y + ki * offset_y))
            d_x = kd * (offset_x - prev_offset_x)
            d_y = kd * (offset_y - prev_offset_y)

            # Calculate control signals
            control_x = p_x + integral_x + d_x
            control_y = p_y + integral_y + d_y

            # Apply non-linear response for small movements
            control_x = np.sign(control_x) * (abs(control_x) ** 0.8)
            control_y = np.sign(control_y) * (abs(control_y) ** 0.8)

            # Update tracking point (where we're zooming towards)
            if abs(offset_x) > movement_threshold or abs(offset_y) > movement_threshold:
                # Gradually move tracking point towards face center
                tracking_x = int(tracking_x * 0.8 + face_center_x * 0.2)
                tracking_y = int(tracking_y * 0.8 + face_center_y * 0.2)
                consecutive_tracking_frames = 0
            else:
                # Face is centered within threshold
                consecutive_tracking_frames += 1
                tracking_x = center_x
                tracking_y = center_y

            # Store current offsets for next iteration
            prev_offset_x = offset_x
            prev_offset_y = offset_y

            # ===== IMPROVED ZOOM LOGIC BASED ON INITIAL FACE SIZE =====
            if initial_face_size is not None and current_time - last_zoom_time > zoom_interval:
                # Calculate face size ratio compared to initial face size
                size_ratio = face_size / initial_face_size

                # Store previous target for comparison
                prev_target_zoom = target_zoom

                # ZOOM LOGIC:
                # - When face is at initial size (ratio = 1.0): zoom = 1.25x
                # - When face is larger/closer (ratio > 1.0): zoom approaches 1.0x
                # - When face is smaller/farther (ratio < 1.0): zoom approaches 3.0x

                # Create a smooth mapping function for each case
                if size_ratio >= 1.0:  # Face is at initial size or larger (person is same distance or closer)
                    # Map range: ratio 1.0 → 2.0 maps to zoom 1.25 → 1.0
                    # This creates a smooth transition from 1.25x to 1.0x as face gets larger
                    target_zoom = max(1.0, 1.25 - 0.25 * min(1.0, (size_ratio - 1.0)))
                else:  # Face is smaller than initial (person is farther away)
                    # Map range: ratio 1.0 → 0.5 maps to zoom 1.25 → 3.0
                    # This creates a smooth transition from 1.25x to 3.0x as face gets smaller
                    target_zoom = min(3.0, 1.25 + (3.0 - 1.25) * min(1.0, (1.0 - size_ratio) / 0.5))

                # Round target to 2 decimal places
                target_zoom = round(target_zoom, 2)

                # Add hysteresis to prevent oscillation - only change zoom if difference is significant
                if abs(target_zoom - zoom_factor) > 0.08:
                    # Smoothly adjust zoom factor
                    zoom_factor = zoom_factor * 0.85 + target_zoom * 0.15
                    zoom_factor = round(zoom_factor, 2)
                    last_zoom_time = current_time

                # Add visual feedback on the frame
                zoom_status = ""
                if size_ratio > 1.05:
                    zoom_status = "Zooming OUT - Person closer"
                elif size_ratio < 0.95:
                    zoom_status = "Zooming IN - Person farther"
                else:
                    zoom_status = "Maintaining zoom - Ideal distance"

                cv2.putText(display_frame, zoom_status, (10, 180),
                            cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 200, 255), 2)

                # Display the face size ratio
                cv2.putText(display_frame, f"Size ratio: {size_ratio:.2f}", (10, 210),
                            cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)

            # Display face size info
            cv2.putText(display_frame, f"Face size: {face_size:.1f}", (10, 30),
                        cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)

            # Display initial face size if calibrated
            if initial_face_size is not None:
                cv2.putText(display_frame, f"Initial size: {initial_face_size:.1f}", (10, 150),
                            cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)
        else:
            frames_without_face += 1
            # If no face is detected, gradually move tracking point back to center
            tracking_x = int(tracking_x * 0.9 + center_x * 0.1)
            tracking_y = int(tracking_y * 0.9 + center_y * 0.1)

        # Display zoom factor
        cv2.putText(display_frame, f"Zoom: {zoom_factor:.2f}x", (10, 60),
                    cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)

        if frames_without_face > 10:
            mode_text = "Searching for Face"
        elif consecutive_tracking_frames > 10:
            mode_text = "Face Centered"
        else:
            mode_text = "Tracking Mode"

        cv2.putText(display_frame, mode_text, (10, 120),
                    cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 255), 2)

        # Apply the simulated zoom effect to the frame
        if initial_face_size is not None:
            zoomed_frame = apply_zoom(original_frame, zoom_factor, tracking_x, tracking_y)
        else:
            # Before calibration, just use default zoom on the center
            zoomed_frame = apply_zoom(original_frame, 1.25)

        # Add same info text to zoomed frame
        cv2.putText(zoomed_frame, f"Zoom: {zoom_factor:.2f}x", (10, 60),
                    cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)

        cv2.putText(zoomed_frame, mode_text, (10, 120),
                    cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 255), 2)

        # Create a side-by-side display
        combined_width = original_width * 2
        combined_frame = np.zeros((original_height, combined_width, 3), dtype=np.uint8)
        combined_frame[:, :original_width] = display_frame
        combined_frame[:, original_width:] = zoomed_frame

        # Add dividing line
        cv2.line(combined_frame,
                 (original_width, 0),
                 (original_width, original_height),
                 (255, 255, 255), 2)

        # Add labels
        cv2.putText(combined_frame, "Original with Tracking", (10, original_height - 20),
                    cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 2)
        cv2.putText(combined_frame, "Zoomed View", (original_width + 10, original_height - 20),
                    cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 2)

        try:
            cv2.imshow("Face Tracking with Zoom", combined_frame)
            key = cv2.waitKey(1) & 0xFF
            if key == ord('q'):
                print("Quitting application")
                break
            elif key == ord('r'):
                print("Resetting zoom and calibration...")
                zoom_factor = 1.25
                target_zoom = zoom_factor
                initial_face_size = None
                initial_face_size_samples = []
                face_size_stabilization_counter = 0
                tracking_x = center_x
                tracking_y = center_y
                print("Face size calibration reset")
            elif key == ord('c'):
                if largest_face is not None:
                    x, y, w, h = largest_face
                    face_size = np.sqrt(w ** 2 + h ** 2)
                    initial_face_size = face_size
                    print(f"Manual calibration: Initial face size set to {initial_face_size:.2f}")
                    zoom_factor = 1.25
                    target_zoom = 1.25

        except Exception as e:
            print(f"Error in main processing loop: {e}")
            continue

    print("Shutting down face tracking system...")
    cap.release()
    cv2.destroyAllWindows()


if __name__ == "__main__":
    try:
        main()
    except KeyboardInterrupt:
        print("\nProgram interrupted by user.")
    except Exception as e:
        print(f"Unhandled error: {e}")
    finally:
        cv2.destroyAllWindows()
        print("Cleanup complete. Goodbye!")